{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "text = \"I want to make a energy efficient touch screen display. I want make it as sustainable as possible. How can I make that? Suggest me relevant patent that I can use\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 14:51:31] Invalid gpu_ids format. Expected a string or a list of ints.\n",
      "[codecarbon INFO @ 14:51:31] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 14:51:31] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 14:51:31] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 14:51:31] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 14:51:31] No CPU tracking mode found. Falling back on CPU constant mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of code time:  1717854691.4556723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 14:51:32] We saw that you have a Intel(R) Xeon(R) Gold 6330 CPU @ 2.00GHz but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 14:51:32] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6330 CPU @ 2.00GHz\n",
      "[codecarbon INFO @ 14:51:32] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 14:51:32]   Platform system: Linux-5.4.0-182-generic-x86_64-with-glibc2.31\n",
      "[codecarbon INFO @ 14:51:32]   Python version: 3.12.1\n",
      "[codecarbon INFO @ 14:51:32]   CodeCarbon version: 2.4.1\n",
      "[codecarbon INFO @ 14:51:32]   Available RAM : 503.339 GB\n",
      "[codecarbon INFO @ 14:51:32]   CPU count: 112\n",
      "[codecarbon INFO @ 14:51:32]   CPU model: Intel(R) Xeon(R) Gold 6330 CPU @ 2.00GHz\n",
      "[codecarbon INFO @ 14:51:32]   GPU count: 2\n",
      "[codecarbon INFO @ 14:51:32]   GPU model: 2 x NVIDIA A40\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.01s/it]\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "[codecarbon INFO @ 14:51:47] Energy consumed for RAM : 0.000609 kWh. RAM Power : 188.75217247009277 W\n",
      "[codecarbon INFO @ 14:51:47] Energy consumed for all GPUs : 0.000497 kWh. Total GPU Power : 153.99845359002887 W\n",
      "[codecarbon INFO @ 14:51:47] Energy consumed for all CPUs : 0.000137 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 14:51:47] 0.001243 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to make a energy efficient touch screen display. I want make it as sustainable as possible. How can I make that? Suggest me relevant patent that I can use \n",
      " What is the main theme of the topic and Write it in quotation after writing Theme: \n",
      "Theme: Energy Efficient and Sustainable Touch Screen Display\n",
      "\n",
      "Patent: US20170270970A1\n",
      "\n",
      "Background of the In\n",
      "Emissions for this code execution is:  0.0003246540035412949\n",
      "End of code time:  1717854707.5364213\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"Start of code time: \",time.time())\n",
    "\n",
    "tracker = EmissionsTracker()\n",
    "tracker.start()\n",
    "\n",
    "\n",
    "\n",
    "model_name_or_path = \"/home/c7361293/triz_llm/llama-2-7b-chat/model\"\n",
    "pt_model_path = \"/home/c7361293/triz_llm/llama-2-7b-chat/consolidated.00.pth\"  # Path to your .pt or .pth file\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path, state_dict=torch.load(pt_model_path))\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name_or_path)\n",
    "model = LlamaForCausalLM.from_pretrained(model_name_or_path)\n",
    "    # Check and set the padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Typically the EOS token can be used as pad token\n",
    "\n",
    "prompt = f\"{text} \\n What is the main theme of the topic and Write it in quotation after writing Theme:\"\" \"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "output_sequences = model.generate(\n",
    "        input_ids=inputs['input_ids'], \n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=100, \n",
    "        num_return_sequences=1,\n",
    "        do_sample=False  # Ensures more deterministic outputs\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "\n",
    "emissions: float = tracker.stop()\n",
    "print(\"Emissions for this code execution is: \", emissions)\n",
    "print(\"End of code time: \",time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codecarbon import EmissionsTracker\n",
    "import time\n",
    "\n",
    "print(\"Start of code time: \",time.time())\n",
    "\n",
    "tracker = EmissionsTracker()\n",
    "tracker.start()\n",
    "\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "prompt = f\"{text} \\n What is the main theme of the topic and Write it in quotation after writing Theme:\"\" \"\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Typically the EOS token can be used as pad token\n",
    "\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "output_sequences = model.generate(\n",
    "    input_ids=inputs['input_ids'], \n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    # max_length=5000,  #Used for abstract summary\n",
    "    max_new_tokens = 500, #Used for Claim summary (Output length = input length + max_new_token)\n",
    "    num_return_sequences=1,\n",
    "    do_sample=False  # Ensures more deterministic outputs\n",
    ")\n",
    "\n",
    "# print(tokenizer.decode(output_sequences, skip_special_tokens=True))\n",
    "text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "emissions: float = tracker.stop()\n",
    "print(\"Emissions for this code execution is: \", emissions)\n",
    "print(\"End of code time: \",time.time())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triztest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
